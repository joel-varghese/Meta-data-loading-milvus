{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6650eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde600cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joeljvarghese/Documents/Workspace/Milvus_ollama_trial/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "base_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "pipe = pipeline(\"conversational\", model=base_model, tokenizer=tokenizer, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2639b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = SentenceTransformer(model_name, device=DEVICE)\n",
    "\n",
    "def emb_text(text):\n",
    "    embeddings = encoder.encode(text, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59c6cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "[ 0.01152764  0.02975923  0.00379159  0.03594419 -0.01584916 -0.01495779\n",
      " -0.01805204 -0.00275517  0.03082995  0.03400182]\n"
     ]
    }
   ],
   "source": [
    "test_embedding = emb_text(\"This is a test\")\n",
    "embedding_dim = len(test_embedding)\n",
    "print(embedding_dim)\n",
    "print(test_embedding[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed45de03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 20:50:07,104 - INFO - detected formats: [<InputFormat.MD: 'md'>]\n",
      "2025-10-22 20:50:07,116 - INFO - Going to convert document batch...\n",
      "2025-10-22 20:50:07,117 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-10-22 20:50:07,132 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-22 20:50:07,137 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-22 20:50:07,138 - INFO - Processing document overview.md\n",
      "2025-10-22 20:50:07,296 - INFO - Finished converting document overview.md in 0.46 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Milvus is a bird of prey in the genus Milvus of the hawk family Accipaitridae, celebrated for its speed in flight, keen vision, and remarkable adaptability.\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "Zilliz adopts the name Milvus for its open-source high-performance, highly scalable vector database that runs efficiently across a wide range of environments, from a laptop to large-scale distributed systems. It is available as both open-source software and a cloud service.\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "Developed by Zilliz and soon donated to the LF AI & Data Foundation under the Linux Foundation, Milvus has become one of the world's leading open-source vector database projects. It is distributed under the Apache 2.0 license, and most contributors are experts from the high-performance computing (HPC) community, specializing in building large-scale systems and optimizing hardware-aware code. Core contributors include professionals from Zilliz, ARM, NVIDIA, AMD, Intel, Meta, IBM, Salesforce, Alibaba, and Microsoft.\n",
      "--------------------------------------------------\n",
      "Chunk 4:\n",
      "Interestingly, every Zilliz open-source project is named after a bird, which is a naming convention that symbolizes freedom, foresight, and the agile evolution of technology.\n",
      "--------------------------------------------------\n",
      "Chunk 5:\n",
      "Unstructured data, such as text, images, and audio, varies in format and carries rich underlying semantics, making it challenging to analyze. To manage this complexity, embeddings are used to convert unstructured data into numerical vectors that capture its essential characteristics. These vectors are then stored in a vector database, enabling fast and scalable searches and analytics.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling_core.transforms.chunker import HierarchicalChunker\n",
    "\n",
    "converter = DocumentConverter()\n",
    "chunker = HierarchicalChunker()\n",
    "\n",
    "source = \"https://milvus.io/docs/overview.md\"\n",
    "doc = converter.convert(source).document\n",
    "\n",
    "texts = [chunk.text for chunk in chunker.chunk(doc)]\n",
    "\n",
    "for i, text in enumerate(texts[:5]):\n",
    "    print(f\"Chunk {i+1}:\\n{text}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d5de11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymilvus: 2.5.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joeljvarghese/Documents/Workspace/Milvus_ollama_trial/.venv/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761181869.216734  264240 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import pymilvus\n",
    "from pymilvus import connections\n",
    "\n",
    "print(f\"pymilvus: {pymilvus.__version__}\")\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "from pymilvus import MilvusClient\n",
    "mc = MilvusClient(\"milvus_demo.db\")\n",
    "\n",
    "collection_name = \"my_rag_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e449064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mc.has_collection(collection_name):\n",
    "    mc.drop_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d09c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.create_collection(\n",
    "    collection_name = collection_name,\n",
    "    dimension=embedding_dim,\n",
    "    metric_type=\"IP\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd0737a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.30s/it]/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]  3.33s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]  2.41s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]  1.53s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]  1.01s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]  1.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]  1.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.87it/s]  2.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]  2.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]  2.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s],  2.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s],  2.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.02it/s],  3.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s],  3.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s],  3.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s],  3.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.51it/s],  3.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.20it/s],  3.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.29it/s],  4.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.56it/s],  4.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.88it/s],  4.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s],  4.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s],  2.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s],  3.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.88it/s],  2.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s],  3.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s],  3.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s],  2.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s],  1.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s],  1.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.61it/s],  1.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.12it/s],  2.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.32it/s],  2.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.76it/s],  2.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.90it/s],  3.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s],  3.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s],  4.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s],  3.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s],  3.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.55it/s],  2.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.90it/s],  2.89it/s]\n",
      "Processing chunks: 100%|██████████| 41/41 [00:18<00:00,  2.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'insert_count': 41, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40], 'cost': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "for i, chunk in enumerate(tqdm(texts, desc=\"Processing chunks\")):\n",
    "    embedding = emb_text(chunk)\n",
    "    data.append({\"id\": i, \"vector\": embedding, \"text\": chunk})\n",
    "\n",
    "mc.insert(collection_name=collection_name, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48cfd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "    \"What are the three deployment modes of Milvus, and what are their differences\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81b47f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.31s/it]\n"
     ]
    }
   ],
   "source": [
    "search_res = mc.search(\n",
    "    collection_name=collection_name,\n",
    "    data=[emb_text(question)],\n",
    "    limit=3,\n",
    "    search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
    "    output_fields=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7082d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"Milvus offers three deployment modes, covering a wide range of data scales-from local prototyping in Jupyter Notebooks to massive Kubernetes clusters managing tens of billions of vectors:\",\n",
      "        0.7934890389442444\n",
      "    ],\n",
      "    [\n",
      "        \"- Milvus Lite is a Python library that can be easily integrated into your applications. As a lightweight version of Milvus, it's ideal for quick prototyping in Jupyter Notebooks or running on edge devices with limited resources. [Learn more](/docs/milvus_lite.md) .\\n- Milvus Standalone is a single-machine server deployment, with all components bundled into a single Docker image for convenient deployment. [Learn more](/docs/install_standalone-docker.md) .\\n- Milvus Distributed can be deployed on Kubernetes clusters, featuring a cloud-native architecture designed for billion-scale or even larger scenarios. This architecture ensures redundancy in critical components. [Learn more](/docs/install_cluster-milvusoperator.md) .\",\n",
      "        0.7534335851669312\n",
      "    ],\n",
      "    [\n",
      "        \"- **High Performance at Scale and High Availability** Milvus features a [distributed architecture](/docs/architecture_overview.md) that separates [compute](/docs/data_processing.md#Data-query) and [storage](/docs/data_processing.md#Data-insertion) . Milvus can horizontally scale and adapt to diverse traffic patterns, achieving optimal performance by independently increasing query nodes for read-heavy workload and data node for write-heavy workload. The stateless microservices on K8s allow [quick recovery](/docs/coordinator_ha.md#Coordinator-HA) from failure, ensuring high availability. The support for [replicas](/docs/replica.md) further enhances fault tolerance and throughput by loading data segments on multiple query nodes. See [benchmark](https://zilliz.com/vector-database-benchmark-tool) for performance comparison.\\n- **Support for Various Vector Index Types and Hardware Acceleration** Milvus separates the system and core vector search engine, allowing it to support all major vector index types that are optimized for different scenarios, including HNSW, IVF, FLAT (brute-force), SCANN, and DiskANN, with [quantization-based](/docs/index-explained.md) variations and [mmap](/docs/mmap.md) . Milvus optimizes vector search for advanced features such as [metadata filtering](/docs/boolean.md) and [range search](/docs/range-search.md) . Additionally, Milvus implements hardware acceleration to enhance vector search performance and supports GPU indexing, such as NVIDIA's [CAGRA](/docs/gpu-cagra.md) .\\n- **Flexible Multi-tenancy and Hot/Cold Storage** Milvus supports [multi-tenancy](/docs/multi_tenancy.md#Multi-tenancy-strategies) through isolation at database, collection, partition, or partition key level. The flexible strategies allow a single cluster to handle hundreds to millions of tenants, also ensures optimized search performance and flexible access control. Milvus enhances cost-effectiveness with hot/cold storage. Frequently accessed hot data can be stored in memory or on SSDs for better performance, while less-accessed cold data is kept on slower, cost-effective storage. This mechanism can significantly reduce costs while maintaining high performance for critical tasks.\\n- **Sparse Vector for Full Text Search and Hybrid Search** In addition to semantic search through dense vector, Milvus also natively supports [full text search](/docs/full-text-search.md) with BM25 as well as learned sparse embedding such as SPLADE and BGE-M3. Users can store sparse vector and dense vector in the same collection, and define functions to rerank results from multiple search requests. See examples of [Hybrid Search with semantic search + full text search](/docs/full_text_search_with_milvus.md) .\\n- **Data Security and Fine-grain Access Control** Milvus ensures data security by implementing [mandatory user authentication](/docs/authenticate.md) , [TLS encryption](/docs/tls.md) , and [Role-Based Access Control (RBAC)](/docs/rbac.md) . User authentication ensures that only authorized users with valid credentials can access the database, while TLS encryption secures all communications within the network. Additionally, RBAC allows for fine-grained access control by assigning specific permissions to users based on their roles. These features make Milvus a robust and secure choice for enterprise applications, protecting sensitive data from unauthorized access and potential breaches.\",\n",
      "        0.715631902217865\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "retrieved_lines_with_distances = [\n",
    "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
    "]\n",
    "print(json.dumps(retrieved_lines_with_distances, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef594332",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join(\n",
    "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e61e4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippet provided.\n",
    "\"\"\"\n",
    "USER_PROMPT = f\"\"\"\n",
    "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18447998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    ")\n",
    "\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "]\n",
    "conversation = Conversation(messages=messages)\n",
    "result = pipe(conversation)\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Milvus_ollama_trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
